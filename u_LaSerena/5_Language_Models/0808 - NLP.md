## N-gram model
- Teniendo una historia queremos ser capaces de predecir la siguiente palabra:
$$
P[w|h]
$$
$w$ word
$h$ history

Si fuera un 2-gram model entonces estariamos prediciendo conexiones de una palabra tras otra:
$$
P[is|what]
$$
## Construir las probabilidades
Para construir las probabilidades observamos el *corpus* de texto disponible; sin embargo entre mas historia queremos incluir se vuelve exponencialmente mas caro realiazr las computaciones

### Markov Assumption
El pasado cercano de un sistema sera suficiente para definir la evolucion del sistema;

Esto puede no ocurrir siempre, para prediccion de texto es necesario que tengamos algo mas de contexto para poder actuar.

![[Traditional Language Modeling.pdf]]