**Generative Pre-trained Transformer**
- while BERT uses all *encoder* 
- GTP will center around the *decoder*; 
- BERT is autoencoding
- GPT is autoregressive
	- It's predicts future values based on past values

## Training of GPT
- The model can't have a sneak peak into the future
	- for this we modify the *Self Attention Block*
	- *Masked Self Attention* and we have to modify the *Softmax in a way*

## Inference
- Conditional GPT
	- Give initial text with [end of text] and predicts what comes next
- Unconditional GPT
	- Not give anything but [end of text]

For choosing he next word
- *Greedy Search*
	- max probability, always the same word given the same text
	- deterministic
- *Beam Search*
	- Take top words (multiverse search)
		- make biffurcations
		- probability of biffurcations
		- choose the trajectory with higher probability
- *Top k Sample*
	- Multinulli (sample from top K words)
- *Top p Sampling*
	- Multinulli draw from the words that have cummulative probability
- *Temperature*
	- Boltzmann Temperature Distribution for picking the words

## Limits of GPT
Right now it's just 3 matrix multiplication
So we're just giving it more data
at some point it's going to plateu

# Hugging Face
Hub where people share pre trained models; to do fine tuning on
- How to use Hugging Face


