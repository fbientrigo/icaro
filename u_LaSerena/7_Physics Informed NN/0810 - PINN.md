### ODE:
$$
D u + h(x) g(u) = f(x) \Rightarrow u(x)
$$
The differential operator:
$$
D = \sum_i \theta_i \frac{d^i}{dx^i}
$$
- $g(u)$ can be *Linear*, making it easier and analytical solvable
- $f(x)$ is the *forcing* function.

### System ODE
$$
\hat D \vec u + \vec h(x) g(\vec u) = f(\vec x)
$$
### System Partial Differential Equation
$$
\hat D \vec u + \vec h(\textcolor{red}{\vec x}) g(\vec u) = f(\vec x)
$$
- $\vec x$ many dependant variables

## Boundary Conditions / Initial Conditions
Initial condition:
$$
u[x=t=0]=u_i
$$
Boundary condition *Dirichlet boundary condition*
$$
u(x=x_0)=u_b 
$$
*Neumann Boundary Condition*
$$
\frac{du}{dx}|_{x_0} = g_0
$$
## Formalism
$$
x \rightarrow \textcolor{green}{\text{NN}[W]}  \rightarrow u^{NN}
$$
- assume we know all components of the:
- for the $D$:
	- Automatic Differentiation Provides The Derivatives

- Tensorflow Compiles:
	- Calculates the Computational Graph
		- Generates the Derivatives with Automatic Differentiation

Because we know all components except $u^{NN}$:
- Residual
$$
\mathcal R_D = \mathcal D u - f(x)
$$
- Boundary Residual
$$
\mathcal R_B = u^{NN}(t=0) - u_i
$$

#### Loss Function:
$$
\mathcal L = \mathcal R_D^2 + \mathcal R_B^2 + \mathcal R_d^2
$$
- $\mathcal R_d$ is the data residual of data
	- using the data means we are informed
	- we compare our NN prediction of $u^{NN}$ with $u^d$ data
$$
\mathcal R_d = u^{NN}(x_d)-u^d(x_d)
$$

Extra *Symbolic Regression*
- Does Symbolic Regression helps more?
	- gives a family of solution for the data set, not only one function
	- increasing complexity provides increasing accuracy but can get overfitted
		- There's must be a Sweet Spot, but where?
		- Depends on the data disponibility
		- Does it make physical sense?
			- AIFeynmann
## Training
$$
W = \text{argmin}_W \mathcal L
$$
#### Interval is Important
- How to choose $x$ interval
	- regular intervals or randomly
		- Random works better to not get trapped in trivial solutions
	- It may not work outside of the training interval
		- Bad Generalization
	- *Reinforcement Learning* can work to where to sample


## Library: NeuroDiffGym/nerodiffeq

## Pro's
- PINN's is differentiable
	- FiniteDifference is not always differentiable
	- *Pretty good at inverse problems* because of the back differeniation
- Fast to evaluate
	- FiniteDiff may require interpolation
- Less storage space
	- only weights are different
		- can be save in *pickle*
	- FiniteDiff is an entire mesh of solution (may be TB)
- Parametrizable
	- The parameters of the differential equation
- Can use Transfer Learning
- Won't overfit
	- Every epoch is different data

## Con's
- Slow to train
	- FiniteDiff is direct to compute
- Hard to satisfy boundary conditions on irregular geometry
- Being a NN is hard to tune
	- need's to find the best hyperparameters
- *No error bound estimation*
	- **It's the big problem**
	- FiniteDiff provides an Order of error
___
## Reusability
- Changing initial conditions:
	- Finite Diff: Have to redo all the integration
	- NN: not necessary, can be inputs

# *Bundle Solutions*
Is cool and invertible
$$
\{u, u_i, \theta\} \Rightarrow \textcolor{green}{\text{NN}[W]}  \rightarrow u^{NN}
$$
- arxiv: 2010.05074.pdf
- Having bundle solutions means I have solved everthing in a
- Thermodynamics of Black Hole Example

# Transfer Learning
- Oneshot transfer learning
	- after changing parameters can solve the problem in miliseconds
$$
\{u, u_i, \theta\} \Rightarrow \textcolor{green}{\text{NN}[W]} \rightarrow H\Rightarrow \textcolor{blue}WH = u^{NN}
$$



